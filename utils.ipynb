{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044fc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv \n",
    "\n",
    "## GET THE PATHS FOR ALL THE PATIENTS \n",
    "def get_files(path_root, patients, types):\n",
    "    # in types is signal that you want to extract. e.g. intrinsic,underlying \n",
    "    all_files = []\n",
    "    new_path = []\n",
    "    for i in patients:\n",
    "        new_path = path_root+i\n",
    "        new_path = new_path +'/'\n",
    "        folder_files = os.listdir(new_path)\n",
    "        for j in range(0,len(folder_files)):\n",
    "            file_path =  new_path + folder_files[j]\n",
    "            for file in types:\n",
    "                a = file_path.find(file)\n",
    "                if a >0:\n",
    "                    all_files.append(file_path)\n",
    "    return all_files\n",
    "def read_data(all_files,signal_type):\n",
    "    patients_list = []\n",
    "    data = []\n",
    "    data2 = []\n",
    "    c=1;\n",
    "\n",
    "    for i in range(0,len(all_files)):\n",
    "        new_path = all_files[i] + signal_type\n",
    "        patients_no = int(re.findall(r'\\d+', new_path)[1])  # Extract the patient id for each signal\n",
    "        patients_list.append(patients_no)\n",
    "        signals = {}\n",
    "        \n",
    "        X = []  # The temporary location of the signal\n",
    "        with open(new_path,'r') as f:\n",
    "            signal = csv.reader(f, delimiter=',')\n",
    "            for ROWS in signal:\n",
    "                X.append(float(ROWS[0]))\n",
    "        #signals = {\"Patient %i\" %patients_no : X};\n",
    "        #df = pd.DataFrame(signals)\n",
    "        #data.append(signals)  # \n",
    "        \n",
    "       # signals = pd.DataFrame(data)\n",
    "        #signals = signals.apply(lambda x: pd.Series(x.dropna().values))  # signals organised in a df with the dropped N/A\n",
    "        \n",
    "        data2.append(X)  # a list with all the signals\n",
    "    return data2,patients_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9e1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# def normalise_func(signal, a, b):\n",
    "#     # Normalize signals between -1 and 1 \n",
    "#     # solving system of linear equations one can find the coefficients\n",
    "#     A = np.min(signal)\n",
    "#     B = np.max(signal)\n",
    "#     C = (a-b)/(A-B)\n",
    "#     k = (C*A - a)/C\n",
    "#     return (signal-k)*C\n",
    "\n",
    "#Normalize signals between -1 and 1 \n",
    "def normalise(filtered_signals,a,b):\n",
    "    normalized_signals = []\n",
    "    for i in range (0,len(filtered_signals)):\n",
    "        signal = filtered_signals[i]\n",
    "        A = np.min(signal)\n",
    "        B = np.max(signal)\n",
    "        C = (a-b)/(A-B)\n",
    "        k = (C*A - a)/C\n",
    "        normalized = (signal-k)*C\n",
    "\n",
    "        normalized_signals.append(normalized)\n",
    "    return normalized_signals\n",
    "\n",
    "# def normalise(filtered_signals):\n",
    "#     normalized_signals = []\n",
    "#     for i in range (0,len(filtered_signals)):\n",
    "#         signal = filtered_signals[i]\n",
    "        \n",
    "#         signal_mean = np.mean(signal)\n",
    "#         signal_std = np.std(signal)\n",
    "#         normalized = (signal - signal_mean) / signal_std\n",
    "\n",
    "#         normalized_signals.append(normalized)\n",
    "#     return normalized_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5c69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING THE ECG SIGNALS   \n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "def Savitzky_Golay(signals, fs):\n",
    "    signals_filtered = []\n",
    "    \n",
    "    for i in range(0,len(signals)):\n",
    "        # Savitzky-Golay filter\n",
    "        y_filtered = savgol_filter(signals[i], 11, 2)\n",
    "        signals_filtered.append(y_filtered) \n",
    "    return  signals_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad20c6c",
   "metadata": {},
   "source": [
    "# Down-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8844d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(baseline_segments): #paced_segments\n",
    "    from scipy import signal\n",
    "    for i in range(0,len(baseline_segments)):\n",
    "        baseline_segments[i] = signal.decimate(baseline_segments[i], 4)  # down sample signals to 250\n",
    "\n",
    "    return baseline_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5affacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the noisy signals in multiple signals \n",
    "\n",
    "def cut_noise(signals_filtered,patients_baseline,labels):\n",
    "    signals_new = []\n",
    "    patients_list = []\n",
    "    labels_list = []\n",
    "    for i in range(0,len(signals_filtered)):\n",
    "        signals_new.append(signals_filtered[i])\n",
    "        patients_list.append(patients_baseline[i])\n",
    "        labels_list.append(labels[i])\n",
    "        if i == 8:\n",
    "            part_a = list(signals_filtered[i][:750])\n",
    "            part_b = list(signals_filtered[i][2625:])\n",
    "            \n",
    "            signals_new[i] = part_a; \n",
    "            signals_new.append(part_b)\n",
    "            \n",
    "            patients_list[i] = patients_baseline[i]\n",
    "            patients_list.append(patients_baseline[i])\n",
    "            \n",
    "            labels_list[i] = labels[i]\n",
    "            labels_list.append(labels[i])\n",
    "            del part_a\n",
    "            del part_b\n",
    "                \n",
    "        if i == 10:\n",
    "            part_a = list(signals_filtered[i][:8550]); part_b = list(signals_filtered[i][9950:13250]); part_c = list(signals_filtered[i][13875:])\n",
    "            a = len(signals_new)-1; \n",
    "            signals_new[a]= part_a\n",
    "            patients_list[a] = patients_baseline[i]\n",
    "            labels_list[a] = labels[i]\n",
    "            \n",
    "            signals_new.append(part_b);patients_list.append(patients_baseline[i]); labels_list.append(labels[i])\n",
    "            signals_new.append(part_c);patients_list.append(patients_baseline[i]); labels_list.append(labels[i])\n",
    "            \n",
    "            #signals_filtered[i] = np.array(part_a + part_b + part_c)\n",
    "            \n",
    "    return signals_new, patients_list,labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc469b",
   "metadata": {},
   "source": [
    "# Calibrate signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e68ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize BP signals between SP and DP \n",
    "def calibrate_signals(BP, scale_factor_BP=40):\n",
    "    scaled_BP = []\n",
    "    #scaled_CVP = [],[]   scale_factor_CVP=8\n",
    "    for i in range (0,len(BP)):\n",
    "        scaled_BP.append(scale_factor_BP * np.array(BP[i]))\n",
    "        #scaled_CVP.append(scale_factor_CVP * np.array(CVP[i]))\n",
    "#         a = round(DP[i])\n",
    "#         b = round(SP[i])\n",
    "#         A = np.min(signal)\n",
    "#         B = np.max(signal)\n",
    "#         C = (a-b)/(A-B)\n",
    "#         k = (C*A - a)/C\n",
    "#         normalized = (signal-k)*C\n",
    "    return scaled_BP  #, scaled_CVP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1795b9",
   "metadata": {},
   "source": [
    "# Denoising + baseline correction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4394940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal(X, dwt_transform, cutoff_low, cutoff_high):\n",
    "    coeffs = wavedec(X, dwt_transform)   # wavelet transform 'bior4.4'\n",
    "    # scale 0 to cutoff_low \n",
    "    for ca in range(0,cutoff_low):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    # scale cutoff_high to end\n",
    "    for ca in range(cutoff_high, len(coeffs)):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    Y = pywt.waverec(coeffs, dwt_transform) # inverse wavelet transform\n",
    "    return Y \n",
    "\n",
    "def get_median_filter_width(sampling_rate, duration):\n",
    "    res = int(sampling_rate*duration)\n",
    "    res += ((res%2) - 1) # needs to be an odd number\n",
    "    return res\n",
    "\n",
    "def filter_signal(X,mfa):\n",
    "    X0 = X  #read orignal signal\n",
    "    for mi in range(0,len(mfa)):\n",
    "        X0 = medfilt(X0,mfa[mi]) # apply median filter one by one on top of each other\n",
    "    X0 = np.subtract(X,X0)  # finally subtract from orignal signal\n",
    "    return X0\n",
    "\n",
    "def denoise(ECG, fs):\n",
    "    # Baseline correction\n",
    "    ms_flt_array = [0.2,0.6]    #<-- length of baseline fitting filters (in seconds)\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype='int')\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(fs,ms_flt_array[i])\n",
    "\n",
    "    ECG_denoised = []\n",
    "    for j in range(len(ECG)):\n",
    "        # denoise signal using DWT \n",
    "        signal_den = denoise_signal(ECG[j],'bior4.4' , 1 , 12) #<--- trade off - the less the cutoff - the more R-peak morphology is lost\n",
    "        signal_flt = filter_signal(signal_den,mfa) \n",
    "        \n",
    "        ECG_denoised.append(signal_flt)\n",
    "    return ECG_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da1a7c",
   "metadata": {},
   "source": [
    "# GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, labels, real, fake, device):\n",
    "    BATCH_SIZE, H, W = real.shape\n",
    "\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1)).repeat(1, H, W).to(device)\n",
    "    \n",
    "    # Interpolate between real and fake data\n",
    "    interpolates = real * alpha + fake * (1 - alpha)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolates,labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the input signals\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolates,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a821f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_unique(list_in):\n",
    "   # intilize an empty list\n",
    "   unq_list = []\n",
    "\n",
    "   # Check for elements\n",
    "   for x in list_in:\n",
    "      # check if exists in unq_list\n",
    "      if x not in unq_list:\n",
    "         unq_list.append(x)\n",
    "         # print list\n",
    "   for x in unq_list:\n",
    "      print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcee85d",
   "metadata": {},
   "source": [
    "# Signals calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50195ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize BP signals between SP and DP \n",
    "def calibrate_signals(BP, CVP, scale_factor_BP=40, scale_factor_CVP=8):\n",
    "    scaled_BP, scaled_CVP = [],[]\n",
    "    for i in range (0,len(BP)):\n",
    "        scaled_BP.append(scale_factor_BP * np.array(BP[i]))\n",
    "        scaled_CVP.append(scale_factor_CVP * np.array(CVP[i]))\n",
    "#         a = round(DP[i])\n",
    "#         b = round(SP[i])\n",
    "#         A = np.min(signal)\n",
    "#         B = np.max(signal)\n",
    "#         C = (a-b)/(A-B)\n",
    "#         k = (C*A - a)/C\n",
    "#         normalized = (signal-k)*C\n",
    "    return scaled_BP, scaled_CVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac6f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
